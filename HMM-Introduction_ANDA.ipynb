{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "5918355f-c759-41e8-9cc9-64baf78695b3"
    }
   },
   "source": [
    "# Fitting a Hidden Markov Model\n",
    "#### Excerpted/condensed/expanded from SSM/notebooks\n",
    "####  Authors: David Wyrick, Luca Mazzucato, Scott Linderman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "2b6476b4-bceb-48bc-8957-e943d943c162"
    }
   },
   "source": [
    "We're going to show how to fit a HMM using the State-Space-Modelling code written by [Scott Linderman](https://github.com/lindermanlab/ssm). To start, we will generate synthetic data from a ground-truth instantiation of the HMM class, and fit another HMM to that generated data. A HMM with gaussian observitions is the simplest graphical model available in _SSM_, and is the building block for more complicated HMMs like Auto-Regressive HMMs. We will start with this model and then move on to fitting a HMM with poisson observations. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "346a61a3-9216-480d-b5b8-39a78782a8c3"
    }
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "import autograd.numpy.random as npr\n",
    "npr.seed(3)\n",
    "\n",
    "import ssm\n",
    "from ssm.util import find_permutation\n",
    "from ssm.plots import gradient_cmap, white_to_color_cmap\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "color_names = [\n",
    "    \"windows blue\",\n",
    "    \"red\",\n",
    "    \"amber\",\n",
    "    \"faded green\",\n",
    "    \"dusty purple\",\n",
    "    \"orange\"\n",
    "    ]\n",
    "\n",
    "colors = sns.xkcd_palette(color_names)\n",
    "cmap = gradient_cmap(colors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "e6b9c054-f24c-4271-85b5-0a8e795dc333"
    }
   },
   "source": [
    "## Overview\n",
    "\n",
    "An HMM consists of a set of hidden state variable, $z$, which can take on one of $K$ values (for our purposes, HMMs will always have discrete states), along with a set of transition probabilities for how the hidden state evolves over time. \n",
    "In other words, we have $z_t \\in \\{1, \\ldots, K\\}$, where $z_t = k$ denotes that the hidden variable is in state $k$ at time $t$.\n",
    "\n",
    "\n",
    "The key assumption in an HMM is that only the most recent state affects the next state. In mathematical terms:\n",
    "\n",
    "$$\n",
    "p(z_t \\mid z_{t-1}, z_{t-2}, \\ldots, z_1) = p(z_t \\mid z_{t-1})\n",
    "$$\n",
    "\n",
    "In an HMM, we don't observe the state itself. Instead, we get a noisy observation of the state at each time step according to some observation model. We'll use $x_t$ to denote the observation at time step $t$. The observation can be a vector or scalar. We'll use $D$ to refer to the dimensionality of the observation. A few of the supported observation models are:\n",
    "\n",
    "1. **Gaussian**: Each discrete state $z_t = k$ is associated with a $D$-dimensional mean $\\mu_k$ and covariance matrix $\\Sigma_k$. Each observation $z_t$ comes from a Gaussian distribution centered at the associated mean, with the corresponding covariance.\n",
    "\n",
    "2. **Poisson**: Each discrete state $z_t = k$ is associated with a $D$-dimensional rate $\\lambda_k$ that determines the probability of observing x events (spikes) with each time window for any given unit (neuron).\n",
    "\n",
    "Note: _SSM_ supports many other observation models for HMMs. Check them out in the source code (observations.py)! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "564edd16-a99d-4329-8e31-98fe1e1cef79"
    }
   },
   "outputs": [],
   "source": [
    "##===== Let's create a ground-truth HMM from which to generate data from =====##\n",
    "# Set the parameters of the HMM\n",
    "time_bins = 200   # number of time bins\n",
    "num_states = 5    # number of discrete states\n",
    "obs_dim = 2       # dimensionality of observation\n",
    "\n",
    "# Make an HMM\n",
    "true_hmm = ssm.HMM(num_states, obs_dim, observations=\"gaussian\")\n",
    "\n",
    "# Manually tweak the means to make them farther apart\n",
    "thetas = np.linspace(0, 2 * np.pi, num_states, endpoint=False)\n",
    "true_hmm.observations.mus = 3 * np.column_stack((np.cos(thetas), np.sin(thetas)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##===== Use HMM as a generative model =====##\n",
    "time_bins = 1000\n",
    "true_states, synthetic_data = true_hmm.sample(time_bins)\n",
    "\n",
    "#As a baseline, let's calculate the log proability of the synthetic data given the current model parameters\n",
    "true_ll = true_hmm.log_probability(synthetic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "c9b4a46a-2f86-4b7f-adb6-70c667a1ac67"
    }
   },
   "outputs": [],
   "source": [
    "##===== Plot the observation distributions =====##\n",
    "lim = .85 * abs(synthetic_data).max()\n",
    "\n",
    "#create mesh grid\n",
    "XX, YY = np.meshgrid(np.linspace(-lim, lim, 100), np.linspace(-lim, lim, 100))\n",
    "data = np.column_stack((XX.ravel(), YY.ravel()))\n",
    "\n",
    "#Get log-likelihood of mesh grid so we can plot the probability density\n",
    "lls = true_hmm.observations.log_likelihoods(data, np.zeros((data.shape[0], 0)), np.ones_like(data, dtype=bool), None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "a201a5b1-0cff-4e1f-9367-c25a89ebac41"
    }
   },
   "source": [
    "Below, we plot the samples obtained from the HMM, color-coded according to the underlying state. The solid curves show regions of of equal probability density around each mean. The thin gray lines trace the latent variable as it transitions from one state to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "0feabc13-812b-4d5e-ac24-f8327ecb4d27"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "for k in range(num_states):\n",
    "    plt.contour(XX, YY, np.exp(lls[:,k]).reshape(XX.shape), cmap=white_to_color_cmap(colors[k]))\n",
    "    plt.plot(synthetic_data[true_states==k, 0], synthetic_data[true_states==k, 1], 'o', mfc=colors[k], mec='none', ms=4)\n",
    "    \n",
    "plt.plot(synthetic_data[:,0], synthetic_data[:,1], '-k', lw=1, alpha=.25)\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.title(\"Observation Distributions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "a58c7a02-2777-4af8-982f-e279bd3bbeb6"
    }
   },
   "source": [
    "Below, we visualize each component of of the observation variable as a time series. The colors correspond to the latent state. The dotted lines represent the \"true\" values of the observation variable (the mean) while the solid lines are the actual observations sampled from the HMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "1ec5ac27-2d23-4660-8702-4156f8ffdf39"
    }
   },
   "outputs": [],
   "source": [
    "##===== Plot the data and latent states over time =====##\n",
    "lim = 1.05 * abs(synthetic_data).max()\n",
    "time_bins = 200\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(true_states[None,:time_bins],\n",
    "           aspect=\"auto\",\n",
    "           cmap=cmap,\n",
    "           vmin=0,\n",
    "           vmax=len(colors)-1,\n",
    "           extent=(0, time_bins, -lim, (obs_dim)*lim))\n",
    "\n",
    "#Get the average value for each state\n",
    "Ey = true_hmm.observations.mus[true_states]\n",
    "\n",
    "#On the same figure, plot the generated data with the latent state which generated it\n",
    "for d in range(obs_dim):\n",
    "    plt.plot(synthetic_data[:,d] + lim * d, '-k')\n",
    "    plt.plot(Ey[:,d] + lim * d, ':k')\n",
    "\n",
    "#Plotting parameters\n",
    "plt.xlim(0, time_bins)\n",
    "plt.xlabel(\"time\")\n",
    "plt.yticks(lim * np.arange(obs_dim), [\"$x_{}$\".format(d+1) for d in range(obs_dim)])\n",
    "plt.title(\"Simulated data from an HMM\")\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "759699ce-fffa-4667-90af-267122e39f01"
    }
   },
   "source": [
    "# Fit an HMM to synthetic data\n",
    "This is all fine, but so far we haven't done anything that useful. It's far more interesting to learn an HMM from data. In the following cells, we'll use the synthetic data we generated above to fit an HMM from scratch. This is done in the following lines:\n",
    "\n",
    "`\n",
    "hmm = ssm.HMM(num_states, obs_dim, observations=\"gaussian\")\n",
    "hmm_lls = hmm.fit(obs, method=\"em\", num_em_iters=N_iters)\n",
    "`\n",
    "\n",
    "In the first line, we create a new HMM instance called `hmm` with a gaussian observation model, as in the previous case. Because we haven't specified anything, the transition probabilities and observation means will be randomly initialized. In the next line, we use the `fit` method to learn the transition probabilities and observation means from data. We set the method to `em` (expectation maximization) and specify the maximum number of iterations which will be used to fit the data. The `fit` method returns a numpy array which shows the log-likelihood of the data over time. We then plot this and see that the EM algorithm quickly converges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "d9064e18-01ca-43d4-a866-1b796cc94297"
    }
   },
   "outputs": [],
   "source": [
    "N_iters = 50\n",
    "num_states = 5    # number of discrete states\n",
    "obs_dim = 2       # dimensionality of observation\n",
    "\n",
    "##===== Create HMM object to fit =====## \n",
    "hmm = ssm.HMM(num_states, obs_dim, observations=\"gaussian\")\n",
    "\n",
    "#This beautiful line of code hides all the business of fitting Hidden Markov models to data under the hood.\n",
    "#Go look at the source code and marvel at how code should be written. \n",
    "hmm_lls = hmm.fit(synthetic_data, method=\"em\", num_iters=N_iters, init_method=\"kmeans\")\n",
    "\n",
    "#Plot log-likelihood over iterations compared to the baseline log-likelihood we calculated earlier\n",
    "plt.plot(hmm_lls, label=\"EM\")\n",
    "plt.plot([0, N_iters], true_ll * np.ones(2), ':k', label=\"True\")\n",
    "plt.xlabel(\"EM Iteration\")\n",
    "plt.ylabel(\"Log Probability\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f9335974-fcee-4a2e-827f-b22a12ed688f"
    }
   },
   "source": [
    "The below cell is a bit subtle. In the first section, we sampled from the HMM and stored the resulting latent state $z$ in a variable called `state`. \n",
    "Now, we are treating our observations from the previous section as data, and seeing whether we can infer the true state given only the observations. However, there is no guarantee that the states we learn correspond to the original states from the true HMM. In order to account for this, we need to find a permutation of the states of our new HMM so that they align with the states of the true HMM from the prior section. This is done in the following two lines:\n",
    "\n",
    "`most_likely_states = hmm.most_likely_states(obs)\n",
    "hmm.permute(find_permutation(true_states, most_likely_states))\n",
    "`  \n",
    "  \n",
    "In the first line, we use the `most_likely_states` method to infer the most likely latent states given the observations.  In the second line we call the `find_permutation` function the permutation that best matches the true state. We then use the `permute` method on our `hmm` instance to permute its states accordingly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "35947156-e3a9-44d6-ab79-aea66d05cda7"
    }
   },
   "outputs": [],
   "source": [
    "# Find a permutation of the states that best matches the true and inferred states\n",
    "#Use viterbi method to detmerine most likely state \n",
    "most_likely_states = hmm.most_likely_states(synthetic_data)\n",
    "hmm.permute(find_permutation(true_states, most_likely_states))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "03d4efcd-66a8-4e0b-8558-6df4658382d4"
    }
   },
   "source": [
    "Below, we plot the inferred states ($z_{\\mathrm{inferred}}$) and the true states ($z_{\\mathrm{true}}$) over time. We see that the two match very closely, but not exactly. The model sometimes has difficulty inferring the state if we only observe that state for a very short time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "84b20c35-4187-4b2b-8287-c99212f17a4b"
    }
   },
   "outputs": [],
   "source": [
    "##===== Exercise =====##\n",
    "# Plot the true and inferred discrete states; How do they compare?\n",
    "hmm_z = hmm.most_likely_states(synthetic_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "747730ff-ab9a-4aff-9da4-6e7b203d2aa6"
    }
   },
   "source": [
    "### Exercise: visualize the Transition Matrices\n",
    "The dynamics of the hidden state in an HMM are specified by the transition probabilities $p(z_t \\mid z_{t-1})$. It's standard to pack these probabilities into a stochastic matrix $A$ where $A_{ij} = p(z_t = j \\mid z_{t-1} = i)$.\n",
    "\n",
    "In SSM, we can access the transition matrices using `hmm.transitions.transition` matrix. In the following two lines, we retrives the transition matrices for the true HMM, as well as the HMM we learned from the data, and compare them visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "67124d1b-c672-47a1-92cc-5538012bcd48"
    }
   },
   "outputs": [],
   "source": [
    "##===== Exercise =====##\n",
    "# Plot the true and inferred discrete states; How do they compare?\n",
    "true_transition_mat = true_hmm.transitions.transition_matrix\n",
    "learned_transition_mat = hmm.transitions.transition_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "\n",
    "This is great and all, but what do you do if you don't know how many latent states are needed to describe the data? Model selection is an iterative process of fitting models with different model parameters on training data and calculating the log-likelihood on heldout data. The goal is to find a peak in the log-likelihood at which point increasing the model parameter of interest (in our case, the # of latent states) actually makes the model fit worse. In practice, log-likelihood plots rarely peak, but plateau very slowly as the model adds more and more parameters. Various methods, including Bayesian Information Criterion, incorporate the # of model parameters into the measure of model selection, effectively putting a penalty on adding more model paramters. \n",
    "\n",
    "$$\n",
    "BIC = k ln(n) - 2 LL\n",
    "$$\n",
    "\n",
    "where k is the # of parameters of the model and n is the # of data points.\n",
    "\n",
    "What we will find is that our model selection does indeed reveal that 5 states is the best to fit our model to the data,which is what we expect! This is shown as the elbow in the curve of the log-likehood (the \"elbow method\" before the plateau) plots and in the minimum in the BIC measure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "##===== K-fold Cross-Validation =====##\n",
    "#There's no avoiding it! \n",
    "\n",
    "#Generate more data for cross-validation\n",
    "time_bins = 10000\n",
    "true_states, synthetic_data = true_hmm.sample(time_bins)\n",
    "\n",
    "#Create kfold cross-validation object which will split data for us\n",
    "nKfold = 5\n",
    "kf = KFold(n_splits=nKfold, shuffle=True, random_state=None)\n",
    "\n",
    "#Just for sanity's sake, let's check how it splits the data\n",
    "#So 5-fold cross-validation uses 80% of the data to train the model, and holds 20% for testing\n",
    "for ii, (train_index, test_index) in enumerate(kf.split(synthetic_data)):\n",
    "    print(f\"kfold {ii} TRAIN:\", len(train_index), \"TEST:\", len(test_index))\n",
    "    \n",
    "#10 different nnumber of latent states\n",
    "#5-fold cross-validation\n",
    "#That's 50 model fits! Thankfully this can be parallelized fairly easily\n",
    "max_states = 10\n",
    "ll_training = np.zeros((max_states,nKfold))\n",
    "ll_heldout = np.zeros((max_states,nKfold))\n",
    "BIC_heldout = np.zeros((max_states,nKfold))\n",
    "\n",
    "hmm_z_ms = np.zeros((max_states,time_bins))\n",
    "#Outer loop over the parameter for which you're doing model selection for\n",
    "for iS, num_states in enumerate(range(1,max_states+1)):\n",
    "    #Number of parameters for the model: (transition matrix) + (mean values for each state) + (covariance matrix for each state)\n",
    "    K = num_states*num_states + num_states*obs_dim + num_states*obs_dim\n",
    "    \n",
    "    #Inner loop over kfolds\n",
    "    for iK, (train_index, test_index) in enumerate(kf.split(synthetic_data)):\n",
    "        nTrain = len(train_index); nTest = len(test_index)#*obs_dim\n",
    "        \n",
    "        #Split data\n",
    "        training_data = synthetic_data[train_index]\n",
    "        test_data = synthetic_data[test_index]\n",
    "        \n",
    "        #Create HMM object to fit\n",
    "        hmm = ssm.HMM(num_states, obs_dim, observations=\"gaussian\")\n",
    "\n",
    "        #fit on training data\n",
    "        hmm_lls = hmm.fit(training_data, method=\"em\", num_iters=N_iters)#, init_method=\"kmeans\")\n",
    "        \n",
    "        #Compute log-likelihood for each dataset\n",
    "        ll_training[iS,iK] = hmm.log_probability(training_data)/nTrain\n",
    "        ll_heldout[iS,iK] = hmm.log_probability(test_data)/nTest\n",
    "        \n",
    "        #Let's calculate the BIC as well for this\n",
    "        BIC_heldout[iS,iK] = K*np.log(nTest) - 2*hmm.log_probability(test_data)\n",
    "        \n",
    "    #Let's do a full model fit and get the state-sequence\n",
    "    hmm = ssm.HMM(num_states, obs_dim, observations=\"gaussian\")\n",
    "    hmm_lls = hmm.fit(synthetic_data, method=\"em\", num_iters=N_iters)\n",
    "    most_likely_states = hmm.most_likely_states(synthetic_data)\n",
    "    \n",
    "    hmm_z_ms[iS,:] = hmm.most_likely_states(synthetic_data)\n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##===== Exercise =====##\n",
    "#Plot the log-likelihood of the training and test datasets, as well as the Bayesian Information Criterion; What do they tell you?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate more data with true_hmm\n",
    "time_bins = 10000\n",
    "true_states, synthetic_data = true_hmm.sample(time_bins)\n",
    "\n",
    "#As a baseline, let's calculate the log proability of the synthetic data given the current model parameters\n",
    "true_ll = true_hmm.log_probability(synthetic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's fit a HMM one more time, but instead of using the verberti method to obtain the most-likely latent state sequence,\n",
    "#we will get the Maximum a posteriori estimation of the latent state sequence. this way, we can impose a probability threshold \n",
    "#and look at only time points where the model is sure it found the right state; this is usually what you want to do because you don't really know if the model is doing a good job capturing the complexity of the data\n",
    "num_states = 5\n",
    "\n",
    "#Let's do a full model fit and get the state-sequence\n",
    "hmm = ssm.HMM(num_states, obs_dim, observations=\"gaussian\")\n",
    "hmm_lls = hmm.fit(synthetic_data, method=\"em\", num_iters=N_iters)\n",
    "\n",
    "#For visualization purposes\n",
    "most_likely_states = hmm.most_likely_states(synthetic_data)\n",
    "hmm.permute(find_permutation(true_states, most_likely_states))\n",
    "most_likely_states = hmm.most_likely_states(synthetic_data)\n",
    "\n",
    "#Look at this one line magic! One line\n",
    "posterior_probabilities, expected_joints, log_likes = hmm.expected_states(synthetic_data)\n",
    "\n",
    "##===== Exercise =====##\n",
    "#Plot the posterior probabilities together and obtain the MAP estimate of the lateatent state sequence\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "e358a229-5f00-4a6c-9b13-011d2afff30c"
    }
   },
   "source": [
    "# Post-Fitting Exercises \n",
    "I always tell people that the hard part about fitting a state-space model to data is not the inference part, but the post-fit analysis and interpretation. The hard part of inference is done for us by Scott Linderman and company in this handy SSM package. What you get out of a successful HMM fit is a state sequence and the associated posterior probabilities. Since it's your data, it's your job to make sense of the segmentation done for you by the HMM. Some questions you can ask: do state transitions occur at specific points in a trial? Or during a particular behavior? Do some states last longer than others? what aspect of the data does each state capture. In the case of of a simple gaussian HMM, this is easy to answer. the mean and variance of each state. For an ARHMM though, what do the AR matrices capture? Similarly for the more complicated state space models.\n",
    "\n",
    "For our simple gaussian HMM, let's look at how well the model did at classifying each datapoint to its corresponding latent state. I chose a random seed at the beginning of the notebook so these distributions would be well separate. If you generated a new \"ground truth\" model from which to produce synthetic data from, you might get overlapping distributions that may not be as easy to separate with another HMM. Try it out! \n",
    "\n",
    "\n",
    "> - State Duration distributions\n",
    "> - State Usage, overall and per trial\n",
    "> - Model Parameters characterizing each state\n",
    "> - State transition points relative to other variables important to an experiment (task variables, behavior, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_data_sub = synthetic_data[:1000]\n",
    "most_likely_states_sub = most_likely_states[:1000]\n",
    "true_states_sub = true_states[:1000]\n",
    "##===== Plot the observation distributions =====##\n",
    "lim = .85 * abs(synthetic_data_sub).max()\n",
    "\n",
    "#create mesh grid\n",
    "XX, YY = np.meshgrid(np.linspace(-lim, lim, 100), np.linspace(-lim, lim, 100))\n",
    "data = np.column_stack((XX.ravel(), YY.ravel()))\n",
    "\n",
    "#Get log-likelihood of mesh grid so we can plot the probability density\n",
    "lls = hmm.observations.log_likelihoods(data, np.zeros((data.shape[0], 0)), np.ones_like(data, dtype=bool), None)\n",
    "\n",
    "fig, axes = plt.subplots(1,2,figsize=(12, 6))\n",
    "\n",
    "ax = axes[0]\n",
    "for k in range(num_states):\n",
    "    ax.contour(XX, YY, np.exp(lls[:,k]).reshape(XX.shape), cmap=white_to_color_cmap(colors[k]))\n",
    "    ax.plot(synthetic_data_sub[most_likely_states_sub==k, 0], synthetic_data_sub[most_likely_states_sub==k, 1], 'o', mfc=colors[k], mec='none', ms=4)\n",
    "    ax.plot(hmm.observations.mus[k].T,'*',color=colors[k])\n",
    "    \n",
    "ax.plot(synthetic_data_sub[:,0], synthetic_data_sub[:,1], '-k', lw=1, alpha=.25)\n",
    "ax.set_xlabel(\"$x_1$\")\n",
    "ax.set_ylabel(\"$x_2$\")\n",
    "ax.set_title(\"Inferred Observation Distributions\")\n",
    "\n",
    "#Get log-likelihood of mesh grid so we can plot the probability density\n",
    "lls = true_hmm.observations.log_likelihoods(data, np.zeros((data.shape[0], 0)), np.ones_like(data, dtype=bool), None)\n",
    "\n",
    "ax = axes[1]\n",
    "for k in range(num_states):\n",
    "    ax.contour(XX, YY, np.exp(lls[:,k]).reshape(XX.shape), cmap=white_to_color_cmap(colors[k]))\n",
    "    ax.plot(synthetic_data_sub[true_states_sub==k, 0], synthetic_data_sub[true_states_sub==k, 1], 'o', mfc=colors[k], mec='none', ms=4)\n",
    "    ax.plot(true_hmm.observations.mus[k].T,'*',color=colors[k])\n",
    "    \n",
    "ax.plot(synthetic_data_sub[:,0], synthetic_data_sub[:,1], '-k', lw=1, alpha=.25)\n",
    "ax.set_xlabel(\"$x_1$\")\n",
    "ax.set_ylabel(\"$x_2$\")\n",
    "ax.set_title(\"True Observation Distributions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poisson HMM\n",
    "Let's do a similar exercise - generating data from a 'ground-truth model' and then fitting a separate HMM to the data, but using a poisson observation model instead of gaussian. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##===== Let's create a ground-truth HMM from which to generate data from =====##\n",
    "# Set the parameters of the HMM\n",
    "time_bins = 1000   # number of time bins\n",
    "num_states = 5     # number of discrete states\n",
    "nNeurons = 100     # number of neurons\n",
    "obs_dim = nNeurons # dimensionality of observation\n",
    "\n",
    "# Make an HMM\n",
    "true_hmm = ssm.HMM(num_states, obs_dim, observations=\"poisson\")\n",
    "\n",
    "#Let's specify the firing rate vectors ourselves\n",
    "true_firing_rates = np.zeros((num_states,nNeurons))\n",
    "\n",
    "#Let's also specify the max firing rate (spikes/time-step, not Hz)\n",
    "max_firing_rates = [20, 30, 5, 25, 15]\n",
    "for k, mFR in enumerate(max_firing_rates):\n",
    "    true_firing_rates[k,:] = np.random.rand(1,nNeurons)*mFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##===== Use HMM as a generative model =====##\n",
    "time_bins = 1000\n",
    "true_states, synthetic_spikes = true_hmm.sample(time_bins)\n",
    "\n",
    "#As a baseline, let's calculate the log proability of the synthetic data given the current model parameters\n",
    "true_ll = true_hmm.log_probability(synthetic_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again let's look at the observations synthetic data and the corresponding latent state sequence which generated it. It should be fairly obvious by eye when a state change occurs. Maybe a bit more obvious than real data :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,1,figsize=(8, 10),gridspec_kw={'height_ratios':[10,1]})\n",
    "\n",
    "tPlot = 250\n",
    "axes[0].imshow(synthetic_spikes[:tPlot].T,\n",
    "           aspect=\"auto\",\n",
    "           cmap='gray_r',\n",
    "           extent=(0, tPlot, 0, nNeurons))\n",
    "\n",
    "axes[0].set_ylabel('Neuron ID')\n",
    "\n",
    "axes[1].imshow(true_states[None,:tPlot], aspect=\"auto\", cmap=cmap, vmin=0, vmax=len(colors)-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Fit poisson HMM to synthetic data and compare the inferred state sequence to that of the true state sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
